# 3. 線形回帰モデル



## 🎯回帰の目標

* $N$個の入力値$\{\bf x \rm_n\}(n=1,...,N)$と目標値$\{t_n\}$からなる訓練データ集合が与えられたとき

  新しい$\bf x$に対する$t$の値を予測すること

### 例

* 100人の身長と体重のデータを用いて、101人目の身長から対応する体重を予測する



## 🔎アプローチ

### 最も単純なアプローチ

* 関数$y({\bf x})$の構築

### 一般的な確率論的観点

* 予測分布$p(t|\bf x \rm)$をモデル化

  * 損失関数の期待値を最小にするように予測できる
  * 実変数に対しては二乗損失関数を選ぶのが一般的
    * 最適解は$t$の条件付き期待値

入力空間が高次元の問題には実用的とは言えないが、解析に的に扱いやすく洗練されたモデルの基礎をなす重要なモデル



## 3.1 線形基底関数モデル

### 最も単純な線形回帰モデル

$$
y({\bf x,\ w}) = w_0 + w_1 x_1 +...+ w_d x_d \tag{3.1}
$$

* 入力変数の線形結合

* **パラメータ$\mathbf{w}$に関する線形関数**

* **入力変数$x_i$に関しても線形関数**

  - 表現能力に乏しいため、このモデルの致命的な問題点

* ${\bf x}  = (x_1,…,x_D)^T$

* $\mathbf{w} = {w_0,…,w_D}$

  

### 線形モデル

​	**<font color='tomato'>入力変数に関して非線形な線形結合だが、パラメータ</font>$\mathbf{w}$<font color='tomato'>に関して線形になる</font>**
$$
y({\bf x,\ w}) = w_0 + \sum_{j=1}^{M-1}w_j\phi({\bf x}) \tag{3.2}
$$

​	ダミーの基底関数$\phi_0 = 1$を追加すると
$$
y({\bf x,\ w}) = \sum_{j=0}^{M-1}w_j\phi_j({\bf x}) = {\bf w}^T\phi({\bf x}). \tag{3.3}
$$

* $\phi({\bf x})$：**<font color='DeepPink'>基底関数(basis function)</font>**

* $M$：パラメータ数

* パラメータ$w_0$：**<font color='DeepPink'>バイアスパラメータ(bias parameter)</font>**

  - データの任意に固定されたオフセット量を許容するもの

* ${\bf w} = (w_0,…,w_M-1)^T$

* $\phi = (\phi_0,…,\phi_M-1)^T$

  

#### 基底関数

* $\phi(x)$で表される
* $\{\phi(\mathbf{x})\}$：特徴ベクトル

##### スプライン関数

<div style='text-align:center;'>
  <img src="/Users/marie/NAIST/PRML/prmlfigs-jpg/Figure3.1a.jpg" alt="Figure3.1a" style="zoom:35%"/>
  <div>
    Fig 3.1a スプライン関数の例
  </div>
</div>

$$
\phi(x) = x^j
$$

* 入力空間を分割し、各領域内で多項式をあてはめる
  * 入力空間のある領域での変化が他の領域に及ばないようにする

##### ガウス基底関数

<div style='text-align:center;'>
  <img src="/Users/marie/NAIST/PRML/prmlfigs-jpg/Figure3.1b.jpg" alt="Figure3.1b" style="zoom:35%"/>
  <div>
    Fig 3.1b スプライン関数の例
  </div>
</div>

$$
\phi_j(x) = {\rm exp}\left\{-\frac{(x-\mu_j)^2}{2s^2}\right\} \tag{3.4}
$$

* $\mu$：入力空間における基底関数の位置
* パラメータ$s$：空間的な尺度
* 基底関数は$w_j$倍されるため、正規化する必要はない

##### シグモイド基底関数

<div style='text-align:center;'>
  <img src="/Users/marie/NAIST/PRML/prmlfigs-jpg/Figure3.1c.jpg" alt="Figure3.1c" style="zoom:35%"/>
  <div>
    Fig 3.1 シグモイド基底関数の例
  </div>
</div>

$$
\phi_j(x) = \sigma\left(\frac{x-\mu_j}{s}\right). \tag{3.5}
$$

<font color='DeepPink'>**ロジスティックシグモイド関数**</font>
$$
\sigma(a) = \frac{1}{1+{\rm exp}(-a)} \tag{3.6}
$$

* ロジスティックシグモイド関数の線形結合とtanh関数の線形結合は等価
  * tanh関数は${\rm tanh}(a)=2\sigma(2a)-1$を満たすため

##### フーリエ基底

* 三角関数展開が得られる

##### ウェーブレット(wavelet)

* 入力区間において局所的な基底関数は必然的に異なる空間的な周波数を含むスペクトルを持つ
* 格子状に並んだ入力値に対して特に使いやすい
  * 一定時間間隔の時系列
  * 画像内の画素



### 3.1.1 最尤推定と最小二乗法

$$
p(t|{\bf x},{\bf w}, \beta) = \mathcal N(t|y({\bf x},{\bf w}),\ \beta^{-1}) \tag{3.8}
$$



#### 目標変数

$$
t = y({\bf x,\ w})+\epsilon. \tag{3.7}
$$

* 決定論的な関数$y({\bf x, \ w})$と、加法性のガウスノイズの和で与えられる
* $\epsilon$：期待値が$0$で精度が$\beta$のガウス確率変数



#### 条件付き期待値

​	二乗損失関数を仮定すると
$$
\mathbb{E}[t|{\bf x}] = \int tp(t|{\bf x})dt = y({\bf x,\ w}) \tag(3.9)
$$

* 新しい${\bf x}$の値に対する最適な予測値は目標変数の条件付き期待値



#### 最尤推定

* 条件付きガウスノイズ分布の下での線形モデルに対する尤度関数を最大化
  * 二乗和誤差関数$E_D(\mathbf{w})$を最小化

##### 尤度関数

$$
p(\mathbf{t}|{\bf X},{\bf w},\beta) = \prod_{n=1}^N\mathcal{N}(t_n|{\bf w}^T\phi({\bf x}_n),\beta^{-1}). \tag{3.10}
$$

* 入力${\bf X}=\{{\bf x}_1,…,{\bf x}_N\}$
* 列ベクトル$\mathbf{t}$：目標変数$\{t_n\}$をまとめたもの

##### 対数尤度関数

$$
\begin{align}
{\rm ln}\ p(t|{\bf w},\beta) = \sum_{n-1}^N\ln\ \mathcal{N}(t_n|{\bf w}^T\phi({\bf x}_n),\beta^{-1}) \\
=\frac{N}{2}\ln\beta\ -\ \frac{N}{2}\ln(2\pi)\ -
 \beta E_D({\bf w}) \tag{3.11}
\end{align}
$$

勾配すると
$$
\nabla\ln p(t|{\bf w},\beta) = \beta\sum_{n=1}^N\{t_n=\{{\bf w}^T\phi(x_n)^T\} \tag{3.13}
$$


###### $\mathbf{w}$についての最尤推定

​	勾配を$0$とおく
$$
0 = \sum_{n=1}^N t_n\phi({\bf x}_n)^T - {\bf w}^T \left(\sum_{n=1}^N\phi({\bf x}_n)\phi({\bf x}_n)^T\right) \tag{3.14}
$$
​	$\mathbf{w}$について解く
$$
{\bf w}_{ML} = \left(\Phi^T\Phi\right)^{-1}\Phi^Tt \tag{3.15}
$$

###### $\beta$についての最尤推定

$$
\frac{1}{\beta_{ML}} = \frac{1}{N}\{t_n-{\bf w}^T_{ML}\phi({\bf x}_n)\}^2 \tag{3.21}
$$



* ノイズの精度の逆数は回帰関数周りでの目標値の残差分散で与えられる

##### 二乗和誤差関数

E_D({\bf w})\ =\ \frac{1}{2}\sum_{n=1}^N\{t_n-{\bf w}^T\phi({\bf x}_n\}^2 \tag{3.12}
$$
E_d({\bf w}) = \frac{1}{2}\sum_{n=1}^N\{t_n-w_0-\sum_{j=1}^{M-1}w_j\phi_j({\bf x}_n\}^2 \tag{3.18}
$$
$w_0$に関する微分を$0$とおき、$w_0$について解けば
$$
w_0 = \bar{t}-\sum_{j=1}^{M-1}w_j\overline{\phi_j} \tag{3.19}
$$

* $$
  \begin{align}
  \bar{t} = \frac{1}{N}\sum_{n=1}^{N}t_n, & \overline{\phi_j} = \frac{1}{N}\sum_{n=1}^{N}\phi_j({\bf x}_n) \tag{3.20}
  \end{align}
  $$

* バイアス$w_0$は目標値の平均と基底関数の値の平均の重み付き和との差を埋め合わせる役割をしている

##### 対数尤度関数の勾配

* 最小二乗問題の正規方程式(normal equation)

* $N \times M$行列$\Phi$は計画行列(design matrix)と呼ばれ、その要素は$\Phi_{nj}=\phi_j({\bf x}_n)$で与えられ
  $$
  \begin{equation}
  \Phi =
  \begin{pmatrix}
  \phi_0({\bf x}_1) &\phi_1({\bf x}_1) &\cdots &\phi_{M-1}({\bf x}_1) \\
  \phi_0({\bf x}_2) &\phi_1({\bf x}_2) &\cdots &\phi_{M-1}({\bf x}_2) \\
  \vdots &\vdots &\ddots &\vdots \\
  \phi_0({\bf x}_N) &\phi({\bf x}_N) &\cdots &\phi_{M-1}({\bf x}_N)
  
  \end{pmatrix}
  \end{equation}
  \tag{3.16}
  $$

##### ムーアーベンローズの擬似逆行列(Mppre-Penrose pseudo-inverse matrix)

$$
\Phi^\dagger \equiv \left(\Phi^T\Phi\right)^{-1}\Phi^T \tag{3.17}
$$

* 通常の逆行列の概念の非正方行列への拡張
* $\Phi$が製法で逆を持つとき、$({\bf AB})^{-1} = {\bf B}^{-1}{\bf A}^{-1}$より$\Phi^\dagger \equiv \Phi^{-1}$が成り立つ